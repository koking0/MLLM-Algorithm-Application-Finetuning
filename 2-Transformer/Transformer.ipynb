{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入向量的维度: torch.Size([1, 10])\n",
      "输出向量的维度: torch.Size([1, 10, 5])\n",
      "最终隐藏状态的维度: torch.Size([1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(input_size=10, hidden_size=5)\n",
    "input_vector = th.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "output, hidden = encoder(input_vector)\n",
    "\n",
    "print(\"输入向量的维度:\", input_vector.size())\n",
    "print(\"输出向量的维度:\", output.size())\n",
    "print(\"最终隐藏状态的维度:\", hidden.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = th.empty(batch_size, 1, dtype=th.long).fill_(\n",
    "            SOS_token)  # Start of Sentence词元，用于表示开始生成一个句子\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(\n",
    "                decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = th.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x = F.relu(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        output = self.out(x)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder = DecoderRNN(hidden_size=5, output_size=10)\n",
    "target_vector = th.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "encoder_outputs, encoder_hidden = encoder(input_vector)\n",
    "output, hidden, _ = decoder(encoder_outputs, encoder_hidden, input_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出向量的维度: torch.Size([1, 10, 10])\n",
      "最终隐藏状态的维度: torch.Size([1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"输出向量的维度:\", output.size())\n",
    "print(\"最终隐藏状态的维度:\", hidden.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(th.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = th.bmm(weights, keys)\n",
    "        return context, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.rnn = nn.RNN(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = th.empty(batch_size, 1, dtype=th.long).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = th.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = th.cat(attentions, dim=1)\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_rnn = th.cat((embedded, context), dim=2)\n",
    "        output, hidden = self.rnn(input_rnn, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder = AttentionDecoderRNN(hidden_size=5, output_size=10)\n",
    "target_vector = th.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "encoder_outputs, encoder_hidden = encoder(input_vector)\n",
    "output, hidden, attentions = decoder(\n",
    "    encoder_outputs, encoder_hidden, input_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出向量的维度: torch.Size([1, 10, 10])\n",
      "注意力权重的维度: torch.Size([1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "print(\"输出向量的维度:\", output.size())\n",
    "print(\"注意力权重的维度:\", attentions.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0999, 0.0906, 0.0837, 0.1180, 0.0767, 0.1011, 0.1099, 0.1038,\n",
       "          0.0949, 0.1213],\n",
       "         [0.0993, 0.0899, 0.0817, 0.1210, 0.0762, 0.0992, 0.1111, 0.1060,\n",
       "          0.0943, 0.1211],\n",
       "         [0.1004, 0.0863, 0.0805, 0.1240, 0.0739, 0.0989, 0.1126, 0.1072,\n",
       "          0.0924, 0.1239],\n",
       "         [0.0993, 0.0898, 0.0815, 0.1213, 0.0760, 0.0990, 0.1113, 0.1061,\n",
       "          0.0942, 0.1214],\n",
       "         [0.1002, 0.0883, 0.0814, 0.1212, 0.0777, 0.0973, 0.1098, 0.1079,\n",
       "          0.0934, 0.1228],\n",
       "         [0.0987, 0.0847, 0.0799, 0.1304, 0.0714, 0.0979, 0.1193, 0.1054,\n",
       "          0.0906, 0.1218],\n",
       "         [0.1002, 0.0903, 0.0822, 0.1189, 0.0770, 0.0991, 0.1090, 0.1071,\n",
       "          0.0950, 0.1213],\n",
       "         [0.0999, 0.0872, 0.0804, 0.1238, 0.0748, 0.0986, 0.1123, 0.1070,\n",
       "          0.0927, 0.1233],\n",
       "         [0.1010, 0.0812, 0.0801, 0.1275, 0.0681, 0.1007, 0.1178, 0.1065,\n",
       "          0.0900, 0.1270],\n",
       "         [0.1010, 0.0854, 0.0817, 0.1227, 0.0721, 0.1012, 0.1126, 0.1066,\n",
       "          0.0926, 0.1240]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DateDataset(Dataset):\n",
    "    def __init__(self, n):\n",
    "        # 初始化两个空列表，用于存储中文和英文日期\n",
    "        self.date_cn = []\n",
    "        self.date_en = []\n",
    "        for _ in range(n):\n",
    "            # 随机生成年份、月份和日期\n",
    "            year = random.randint(1950, 2050)\n",
    "            month = random.randint(1, 12)\n",
    "            day = random.randint(1, 28)  # 假设最大日期为28日\n",
    "            date = datetime.date(year, month, day)\n",
    "            # 格式化日期并添加到对应的列表中\n",
    "            self.date_cn.append(date.strftime(\"%y-%m-%d\"))\n",
    "            self.date_en.append(date.strftime(\"%d/%b/%Y\"))\n",
    "        # 创建一个词汇集，包含0-9的数字、\"-\"、\"/\"和英文日期中的月份缩写\n",
    "        self.vocab = set([str(i) for i in range(0, 10)] +\n",
    "                         [\"-\", \"/\"] + [i.split(\"/\")[1] for i in self.date_en])\n",
    "        # 创建一个词汇到索引的映射，其中\"<SOS>\"、\"<EOS>\"和\"<PAD>\"分别对应开始、结束和填充标记\n",
    "        self.word2index = {v: i for i, v in enumerate(sorted(list(self.vocab)), start=3)}\n",
    "        self.word2index[\"<PAD>\"] = PAD_token\n",
    "        self.word2index[\"<SOS>\"] = SOS_token\n",
    "        self.word2index[\"<EOS>\"] = EOS_token\n",
    "        # 将开始、结束和填充标记添加到词汇集中\n",
    "        self.vocab.add(\"<SOS>\")\n",
    "        self.vocab.add(\"<EOS>\")\n",
    "        self.vocab.add(\"<PAD>\")\n",
    "        # 创建一个索引到词汇的映射\n",
    "        self.index2word = {i: v for v, i in self.word2index.items()}\n",
    "        # 初始化输入和目标列表\n",
    "        self.input, self.target = [], []\n",
    "        for cn, en in zip(self.date_cn, self.date_en):\n",
    "            # 将日期字符串转换为词汇索引列表，然后添加到输入和目标列表中\n",
    "            self.input.append([self.word2index[v] for v in cn])\n",
    "            self.target.append(\n",
    "                [self.word2index[\"<SOS>\"], ] +\n",
    "                [self.word2index[v] for v in en[:3]] +\n",
    "                [self.word2index[en[3:6]]] +\n",
    "                [self.word2index[v] for v in en[6:]] +\n",
    "                [self.word2index[\"<EOS>\"], ]\n",
    "            )\n",
    "        # 将输入和目标列表转换为NumPy数组\n",
    "        self.input, self.target = np.array(self.input), np.array(self.target)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集的长度，即输入的数量\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 返回给定索引的输入、目标和目标的长度\n",
    "        return self.input[index], self.target[index], len(self.target[index])\n",
    "\n",
    "    @property\n",
    "    def num_word(self):\n",
    "        # 返回词汇表的大小\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = DateDataset(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-': 3,\n",
       " '/': 4,\n",
       " '0': 5,\n",
       " '1': 6,\n",
       " '2': 7,\n",
       " '3': 8,\n",
       " '4': 9,\n",
       " '5': 10,\n",
       " '6': 11,\n",
       " '7': 12,\n",
       " '8': 13,\n",
       " '9': 14,\n",
       " 'Apr': 15,\n",
       " 'Aug': 16,\n",
       " 'Dec': 17,\n",
       " 'Feb': 18,\n",
       " 'Jan': 19,\n",
       " 'Jul': 20,\n",
       " 'Jun': 21,\n",
       " 'Mar': 22,\n",
       " 'May': 23,\n",
       " 'Nov': 24,\n",
       " 'Oct': 25,\n",
       " 'Sep': 26,\n",
       " '<PAD>': 0,\n",
       " '<SOS>': 1,\n",
       " '<EOS>': 2}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['84-05-22', '71-09-05', '30-01-09', '32-09-09', '36-11-19']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.date_cn[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22/May/1984', '05/Sep/1971', '09/Jan/2030', '09/Sep/2032', '19/Nov/2036']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.date_en[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  9,  3,  5, 10,  3,  7,  7],\n",
       "       [12,  6,  3,  5, 14,  3,  5, 10],\n",
       "       [ 8,  5,  3,  5,  6,  3,  5, 14],\n",
       "       [ 8,  7,  3,  5, 14,  3,  5, 14],\n",
       "       [ 8, 11,  3,  6,  6,  3,  6, 14]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.input[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  7,  7,  4, 23,  4,  6, 14, 13,  9,  2],\n",
       "       [ 1,  5, 10,  4, 26,  4,  6, 14, 12,  6,  2],\n",
       "       [ 1,  5, 14,  4, 19,  4,  7,  5,  8,  5,  2],\n",
       "       [ 1,  5, 14,  4, 26,  4,  7,  5,  8,  7,  2],\n",
       "       [ 1,  6, 14,  4, 24,  4,  7,  5,  8, 11,  2]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "MAX_LENGTH = 11\n",
    "hidden_size = 128\n",
    "learning_rate=0.001\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "encoder = EncoderRNN(dataset.num_word, hidden_size)\n",
    "decoder = AttentionDecoderRNN(hidden_size, dataset.num_word)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, \tloss: 1.976585115155866\n",
      "epoch: 10, \tloss: 0.010210241776921095\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epochs + 1):\n",
    "    total_loss = 0\n",
    "    for input_tensor, target_tensor, target_length in dataloader:\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(\n",
    "            encoder_outputs, encoder_hidden, target_tensor)\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1).long()\n",
    "        )\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    total_loss /= len(dataloader)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"epoch: {i}, \\tloss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, x):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    encoder_outputs, encoder_hidden = encoder(th.tensor(np.array([x])))\n",
    "    start = th.ones(x.shape[0], 1)    # [n, 1]\n",
    "    start[:, 0] = th.tensor(SOS_token).long()\n",
    "    decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden)\n",
    "    _, topi = decoder_outputs.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "    decoded_words = []\n",
    "    for idx in decoded_ids:\n",
    "        decoded_words.append(dataset.index2word[idx.item()])\n",
    "    return ''.join(decoded_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 84-05-22, target: 22/May/1984, predict: <SOS>22/May/1984<EOS>\n",
      "input: 71-09-05, target: 05/Sep/1971, predict: <SOS>05/Sep/1971<EOS>\n",
      "input: 30-01-09, target: 09/Jan/2030, predict: <SOS>09/Jan/2030<EOS>\n",
      "input: 32-09-09, target: 09/Sep/2032, predict: <SOS>09/Sep/2032<EOS>\n",
      "input: 36-11-19, target: 19/Nov/2036, predict: <SOS>19/Nov/2036<EOS>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    predict = evaluate(encoder, decoder, dataset[i][0])\n",
    "    print(\n",
    "        f\"input: {dataset.date_cn[i]}, target: {dataset.date_en[i]}, predict: {predict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy, softmax, relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, model_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        # 每个注意力头的维度\n",
    "        self.head_dim = model_dim // n_head\n",
    "        # 注意力头的数量\n",
    "        self.n_head = n_head\n",
    "        # 模型的维度\n",
    "        self.model_dim = model_dim\n",
    "        # 初始化线性变换层，用于生成query、key和value\n",
    "        self.wq = nn.Linear(model_dim, n_head * self.head_dim)\n",
    "        self.wk = nn.Linear(model_dim, n_head * self.head_dim)\n",
    "        self.wv = nn.Linear(model_dim, n_head * self.head_dim)\n",
    "        # 输出的全连接层\n",
    "        self.output_dense = nn.Linear(model_dim, model_dim)\n",
    "        # Dropout层，用于防止模型过拟合\n",
    "        self.output_drop = nn.Dropout(drop_rate)\n",
    "        # 层归一化，用于稳定神经网络的训练\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "        self.attention = None\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # 保存原始输入q，用于后续的残差连接\n",
    "        residual = q\n",
    "        # 分别对输入q,k,v做线性变换，生成query、key和value\n",
    "        query = self.wq(q)\n",
    "        key   = self.wk(k)\n",
    "        value = self.wv(v)\n",
    "        # 对生成的query、key和value进行头分割，以便进行多头注意力计算\n",
    "        query = self.split_heads(query)\n",
    "        key   = self.split_heads(key)\n",
    "        value = self.split_heads(value)\n",
    "        # 计算上下文向量\n",
    "        context = self.scaled_dot_product_attention(query, key, value, mask)\n",
    "        # 对上下文向量进行线性变换\n",
    "        output = self.output_dense(context)\n",
    "        # 添加dropout\n",
    "        output = self.output_drop(output)\n",
    "        # 添加残差连接并进行层归一化\n",
    "        output = self.layer_norm(residual + output)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # 将输入x的shape变为(n, step, n_head, head_dim)，然后进行重排，得到(n, n_head, step, head_dim)\n",
    "        x = th.reshape(x, (x.shape[0], x.shape[1], self.n_head, self.head_dim))\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        # 计算缩放因子\n",
    "        dk = th.tensor(k.shape[-1]).type(th.float)\n",
    "        # 计算注意力分数\n",
    "        score = th.matmul(q, k.permute(0, 1, 3, 2)) / (th.sqrt(dk) + 1e-8)\n",
    "        if mask is not None:\n",
    "            # 如果提供了mask，将mask位置的分数设置为负无穷，使得这些位置的softmax值接近0\n",
    "            score = score.masked_fill_(mask,-np.inf)\n",
    "        # 计算softmax得到注意力权重\n",
    "        self.attention = softmax(score,dim=-1)\n",
    "        # 计算上下文向量\n",
    "        context = th.matmul(self.attention,v)\n",
    "        # 重排上下文向量的维度并进行维度合并\n",
    "        context = context.permute(0, 2, 1, 3)\n",
    "        context = context.reshape((context.shape[0], context.shape[1],-1))  \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, model_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # 前馈神经网络的隐藏层维度，设为模型维度的4倍\n",
    "        ffn_dim = model_dim * 4\n",
    "        # 第一个线性变换层，其输出维度为前馈神经网络的隐藏层维度\n",
    "        self.linear1 = nn.Linear(model_dim, ffn_dim)\n",
    "        # 第二个线性变换层，其输出维度为模型的维度\n",
    "        self.linear2 = nn.Linear(ffn_dim, model_dim)\n",
    "        # Dropout层，用于防止模型过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 层归一化，用于稳定神经网络的训练\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对输入x进行前馈神经网络的计算\n",
    "        # 首先，通过第一个线性变换层并使用relu作为激活函数\n",
    "        output = relu(self.linear1(x))\n",
    "        # 然后，通过第二个线性变换层\n",
    "        output = self.linear2(output)\n",
    "        # 接着，对输出做dropout\n",
    "        output = self.dropout(output)\n",
    "        # 最后，对输入x和前馈神经网络的输出做残差连接，然后进行层归一化\n",
    "        output = self.layer_norm(x + output)\n",
    "        return output  # 返回结果，其shape为[n, step, dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_head, emb_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        # 多头注意力机制层\n",
    "        self.mha = MultiHeadAttention(n_head, emb_dim, drop_rate)\n",
    "        # 前馈神经网络层\n",
    "        self.ffn = PositionWiseFFN(emb_dim, drop_rate)\n",
    "\n",
    "    def forward(self, xz, mask):\n",
    "        # xz的形状为 [n, step, emb_dim]\n",
    "        # 通过多头注意力机制层处理xz，得到context，其形状也为 [n, step, emb_dim]\n",
    "        context = self.mha(xz, xz, xz, mask)  \n",
    "        # 将context传入前馈神经网络层，得到输出\n",
    "        output = self.ffn(context)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_head, emb_dim, drop_rate, n_layer):\n",
    "        super().__init__()\n",
    "        # 定义n_layer个EncoderLayer，保存在一个ModuleList中\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(n_head, emb_dim, drop_rate) for _ in range(n_layer)]\n",
    "        )\n",
    "\n",
    "    def forward(self, xz, mask):\n",
    "        # 依次通过所有的EncoderLayer\n",
    "        for encoder in self.encoder_layers:\n",
    "            xz = encoder(xz, mask)\n",
    "        return xz  # 返回的xz形状为 [n, step, emb_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_head, model_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        # 定义两个多头注意力机制层\n",
    "        self.mha = nn.ModuleList([MultiHeadAttention(n_head, model_dim, drop_rate) for _ in range(2)])\n",
    "        # 定义一个前馈神经网络层\n",
    "        self.ffn = PositionWiseFFN(model_dim, drop_rate)\n",
    "\n",
    "    def forward(self, yz, xz, yz_look_ahead_mask, xz_pad_mask):\n",
    "        # 第一个注意力层的计算，三个输入均为yz，使用自注意力机制\n",
    "        dec_output = self.mha[0](yz, yz, yz, yz_look_ahead_mask)  # [n, step, model_dim]\n",
    "        # 第二个注意力层的计算，其中q来自前一个注意力层的输出，K和V来自编码器的输出\n",
    "        dec_output = self.mha[1](dec_output, xz, xz, xz_pad_mask)  # [n, step, model_dim]\n",
    "        # 通过前馈神经网络层\n",
    "        dec_output = self.ffn(dec_output)   # [n, step, model_dim]\n",
    "        return dec_output\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_head, model_dim, drop_rate, n_layer):\n",
    "        super().__init__()\n",
    "        # 定义n_layer个DecoderLayer，保存在一个ModuleList中\n",
    "        self.num_layers = n_layer\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(n_head, model_dim, drop_rate) for _ in range(n_layer)]\n",
    "        )\n",
    "\n",
    "    def forward(self, yz, xz, yz_look_ahead_mask, xz_pad_mask):\n",
    "        # 依次通过所有的DecoderLayer\n",
    "        for decoder in self.decoder_layers:\n",
    "            yz = decoder(yz, xz, yz_look_ahead_mask, xz_pad_mask)\n",
    "        return yz  # 返回的yz形状为 [n, step, model_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, emb_dim, n_vocab):\n",
    "        super().__init__()\n",
    "        # 生成位置编码矩阵\n",
    "        pos = np.expand_dims(np.arange(max_len), 1)  # [max_len, 1]\n",
    "        # 使用正弦和余弦函数生成位置编码\n",
    "        pe = pos / np.power(1000, 2*np.expand_dims(np.arange(emb_dim)//2, 0)/emb_dim)\n",
    "        pe[:, 0::2] = np.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = np.cos(pe[:, 1::2])\n",
    "        pe = np.expand_dims(pe, 0)  # [1, max_len, emb_dim]\n",
    "        self.pe = th.from_numpy(pe).type(th.float32)\n",
    "\n",
    "        # 定义词嵌入层\n",
    "        self.embeddings = nn.Embedding(n_vocab, emb_dim)\n",
    "        # 初始化词嵌入层的权重\n",
    "        self.embeddings.weight.data.normal_(0, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 确保位置编码在与词嵌入权重相同的设备上\n",
    "        device = self.embeddings.weight.device\n",
    "        self.pe = self.pe.to(device)\n",
    "        # 计算输入的词嵌入，并加上位置编码\n",
    "        x_embed = self.embeddings(x) + self.pe  # [n, step, emb_dim]\n",
    "        return x_embed  # [n, step, emb_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_zero(seqs, max_len):\n",
    "    # 初始化一个全是填充标识符PAD_token的二维矩阵，大小为(len(seqs), max_len)\n",
    "    padded = np.full((len(seqs), max_len), fill_value=PAD_token, dtype=np.int32)\n",
    "    for i, seq in enumerate(seqs):\n",
    "        # 将seqs中的每个序列seq的元素填入padded对应的行中，未填满的部分仍为PAD_token\n",
    "        padded[i, :len(seq)] = seq\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_vocab, max_len, n_layer=6, emb_dim=512, n_head=8, drop_rate=0.1, padding_idx=0):\n",
    "        super().__init__()\n",
    "        # 初始化最大长度、填充索引、词汇表大小\n",
    "        self.max_len = max_len\n",
    "        self.padding_idx = th.tensor(padding_idx)\n",
    "        self.dec_v_emb = n_vocab\n",
    "        # 初始化位置嵌入、编码器、解码器和输出层\n",
    "        self.embed = PositionEmbedding(max_len, emb_dim, n_vocab)\n",
    "        self.encoder = Encoder(n_head, emb_dim, drop_rate, n_layer)\n",
    "        self.decoder = Decoder(n_head, emb_dim, drop_rate, n_layer)\n",
    "        self.output = nn.Linear(emb_dim, n_vocab)\n",
    "        # 初始化优化器\n",
    "        self.opt = th.optim.Adam(self.parameters(), lr=0.002)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # 对输入和目标进行嵌入\n",
    "        x_embed, y_embed = self.embed(x), self.embed(y)\n",
    "        # 创建填充掩码\n",
    "        pad_mask = self._pad_mask(x)\n",
    "        # 对输入进行编码\n",
    "        encoded_z = self.encoder(x_embed, pad_mask)\n",
    "        # 创建前瞻掩码\n",
    "        yz_look_ahead_mask = self._look_ahead_mask(y)\n",
    "        # 将编码后的输入和前瞻掩码传入解码器\n",
    "        decoded_z = self.decoder(\n",
    "            y_embed, encoded_z, yz_look_ahead_mask, pad_mask)\n",
    "        # 通过输出层得到最终输出\n",
    "        output = self.output(decoded_z)\n",
    "        return output\n",
    "\n",
    "    def step(self, x, y):\n",
    "        # 清空梯度\n",
    "        self.opt.zero_grad()\n",
    "        # 计算输出和损失\n",
    "        logits = self(x, y[:, :-1])\n",
    "        loss = cross_entropy(logits.reshape(-1, self.dec_v_emb), y[:, 1:].reshape(-1))\n",
    "        # 进行反向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        self.opt.step()\n",
    "        return loss.cpu().data.numpy(), logits\n",
    "\n",
    "    def _pad_bool(self, seqs):\n",
    "        # 创建掩码，标记哪些位置是填充的\n",
    "        return th.eq(seqs, self.padding_idx)\n",
    "\n",
    "    def _pad_mask(self, seqs):\n",
    "        # 将填充掩码扩展到合适的维度\n",
    "        len_q = seqs.size(1)\n",
    "        mask = self._pad_bool(seqs).unsqueeze(1).expand(-1, len_q, -1)\n",
    "        return mask.unsqueeze(1)\n",
    "\n",
    "    def _look_ahead_mask(self, seqs):\n",
    "        # 创建前瞻掩码，防止在生成序列时看到未来的信息\n",
    "        device = next(self.parameters()).device\n",
    "        _, seq_len = seqs.shape\n",
    "        mask = th.triu(th.ones((seq_len, seq_len), dtype=th.long),\n",
    "                       diagonal=1).to(device)\n",
    "        mask = th.where(self._pad_bool(seqs)[:, None, None, :], 1, mask[None, None, :, :]).to(device)\n",
    "        return mask > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, \tloss: 1.6761116981506348\n",
      "epoch: 2, \tloss: 1.1545575857162476\n",
      "epoch: 3, \tloss: 1.027500867843628\n",
      "epoch: 4, \tloss: 0.9820101261138916\n",
      "epoch: 5, \tloss: 0.8658531308174133\n",
      "epoch: 6, \tloss: 0.6056420803070068\n",
      "epoch: 7, \tloss: 0.3543527126312256\n",
      "epoch: 8, \tloss: 0.1970849484205246\n",
      "epoch: 9, \tloss: 0.07973289489746094\n",
      "epoch: 10, \tloss: 0.0385725274682045\n"
     ]
    }
   ],
   "source": [
    "# 初始化一个Transformer模型，设置词汇表大小、最大序列长度、层数、嵌入维度、多头注意力的头数、 dropout比率和填充标记的索引\n",
    "model = Transformer(n_vocab=dataset.num_word, max_len=MAX_LENGTH, n_layer=3, emb_dim=32, n_head=8, drop_rate=0.1, padding_idx=0)\n",
    "# 检测是否有可用的GPU，如果有，则使用GPU进行计算；如果没有，则使用CPU\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "# 将模型移动到相应的设备（CPU或GPU）\n",
    "model = model.to(device)\n",
    "# 创建一个数据集，包含1000个样本\n",
    "dataset = DateDataset(1000)\n",
    "# 创建一个数据加载器，设定批次大小为32，每个批次的数据会被打乱\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# 进行10个训练周期\n",
    "for i in range(10):\n",
    "    # 对于数据加载器中的每批数据\n",
    "    for input_tensor, target_tensor, _ in dataloader:\n",
    "        # 对输入和目标张量进行零填充，使其长度达到最大长度，然后将其转换为PyTorch张量，并移动到相应的设备（CPU或GPU）\n",
    "        input_tensor = th.from_numpy(\n",
    "            pad_zero(input_tensor, max_len=MAX_LENGTH)).long().to(device)\n",
    "        target_tensor = th.from_numpy(\n",
    "            pad_zero(target_tensor, MAX_LENGTH+1)).long().to(device)\n",
    "        # 使用模型的step方法进行一步训练，并获取损失值\n",
    "        loss, _ = model.step(input_tensor, target_tensor)\n",
    "    # 打印每个训练周期后的损失值\n",
    "    print(f\"epoch: {i+1}, \\tloss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x, y):\n",
    "    model.eval()\n",
    "    x = th.from_numpy(pad_zero([x], max_len=MAX_LENGTH)).long().to(device)\n",
    "    y = th.from_numpy(pad_zero([y], max_len=MAX_LENGTH)).long().to(device)\n",
    "    decoder_outputs = model(x, y)\n",
    "    _, topi = decoder_outputs.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "    decoded_words = []\n",
    "    for idx in decoded_ids:\n",
    "        decoded_words.append(dataset.index2word[idx.item()])\n",
    "    return ''.join(decoded_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 59-04-21, target: 21/Apr/1959, predict: 21/Apr/1959<EOS><PAD>\n",
      "input: 69-04-12, target: 12/Apr/1969, predict: 12/Apr/1969<EOS><PAD>\n",
      "input: 22-06-03, target: 03/Jun/2022, predict: 03/Jun/2022<EOS><PAD>\n",
      "input: 70-01-06, target: 06/Jan/1970, predict: 06/Jan/1970<EOS><PAD>\n",
      "input: 35-12-20, target: 20/Dec/2035, predict: 20/Dec/2035<EOS><PAD>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    predict = evaluate(model, dataset[i][0], dataset[i][1])\n",
    "    print(\n",
    "        f\"input: {dataset.date_cn[i]}, target: {dataset.date_en[i]}, predict: {predict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
